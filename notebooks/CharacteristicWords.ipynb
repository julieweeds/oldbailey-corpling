{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristic Words\n",
    "\n",
    "A notebook to explore the distribution of characterising / high frequency / function words in different corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import operator,math\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "\n",
    "import nlp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to find the high frequency (ranked higher than k) distribution for a corpus and display the very highest frequency words (ranked higher than cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a given set of corpora, find the frequency distribution of the k highest frequency words\n",
    "#Output total size of corpus and sorted list of term, frequency pairs\n",
    "\n",
    "def find_hfw_dist(corpora,k=100000):\n",
    "    #add worddicts for individual corpora\n",
    "    #sort and output highest frequency words\n",
    "    #visualise\n",
    "    \n",
    "    sumdict={}\n",
    "    corpussize=0\n",
    "    for acorpus in corpora:\n",
    "        for(key,value) in acorpus.allworddict.items():\n",
    "            sumdict[key.lower()]=sumdict.get(key.lower(),0)+value\n",
    "            corpussize+=value\n",
    "      \n",
    "    print(\"Size of corpus is {}\".format(corpussize))\n",
    "    candidates=sorted(sumdict.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    #print(candidates[:50])\n",
    "    #print(len(sumdict))\n",
    "    #print(sumdict)\n",
    "    return corpussize,candidates[:k]\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display the most frequent words in 1 or 2 high frequency word distributions\n",
    "\n",
    "def display(hfw,cutoff=10,words=[],hfb=None,leg=None):\n",
    "    width=0.7\n",
    "    corpussize=hfw[0]\n",
    "    if words==[]:\n",
    "        todisplay=hfw[1][:cutoff]\n",
    "    else:\n",
    "        todisplay=[(x,y) for (x,y) in hfw[1] if x in words]\n",
    "        cutoff=len(words)\n",
    "    barvalues=sorted(todisplay,key=operator.itemgetter(0),reverse=False)\n",
    "    #print(barvalues)\n",
    "    xs,ys=[*zip(*barvalues)]\n",
    "    ps=[y/corpussize for y in ys]\n",
    "    \n",
    "    toplot=[ps]\n",
    "    if hfb!=None:\n",
    "        width=0.35\n",
    "        corpussizeb=hfb[0]\n",
    "        if words==[]:\n",
    "            todisplayb=hfb[1][:cutoff]\n",
    "        else:\n",
    "            todisplayb=[(x,y) for (x,y) in hfb[1] if x in words]\n",
    "        barb=sorted(todisplayb,key=operator.itemgetter(0),reverse=False)\n",
    "        \n",
    "        xb,yb=[*zip(*barb)]\n",
    "        pb=[y/corpussizeb for y in yb]\n",
    "        \n",
    "        toplot.append(pb)\n",
    "        \n",
    "    \n",
    "    N=len(xs)\n",
    "    ind=np.arange(N)\n",
    "    fig,ax=plt.subplots(figsize=(cutoff,cutoff/4))\n",
    "    \n",
    "    rects1=ax.bar(ind,ps,width,color='r')\n",
    "    if hfb!=None:\n",
    "        rects2=ax.bar(ind+width,pb,width,color='b')\n",
    "        if leg==None:\n",
    "            ax.legend((rects1,rects2),(\"Dist1\",\"Dist2\"))\n",
    "        else:\n",
    "            ax.legend((rects1,rects2),leg)\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(xs)\n",
    "    ax.set_xlabel('High Frequency Words')\n",
    "    ax.set_ylabel('Probability')\n",
    "    \n",
    "    \n",
    "    return xs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Differences Between Female and Male Non-Legal Speech in 1800-1820\n",
    "\n",
    "We are interested in the period 1800-1820.  We are interested in non-legal speech (witness and defendants) in 'theft' trials (which make up the bulk of the trials).  These are split into female and male (by speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b161e0f13cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmale_nonlegal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus_1800_1820_theft_m_def'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'corpus_1800_1820_theft_m_wv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfemale_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfemale_nonlegal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmale_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmale_nonlegal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "p=100\n",
    "female_nonlegal=['corpus_1800_1820_theft_f_def','corpus_1800_1820_theft_f_wv']\n",
    "male_nonlegal=['corpus_1800_1820_theft_m_def','corpus_1800_1820_theft_m_wv']\n",
    "\n",
    "female_corpus=nlp_tools.corpus(female_nonlegal,nlp,prop=p,ner=False)\n",
    "male_corpus=nlp_tools.corpus(male_nonlegal,nlp,prop=p,ner=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the top 20 most frequent words across the whole corpus (i.e., males and males taken together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfw_theft=find_hfw_dist([male_corpus,female_corpus],k=100000)\n",
    "function_words_theft=display(hfw_theft,cutoff=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the distributions of these words for males and females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hff_theft=find_hfw_dist([female_corpus])\n",
    "hfm_theft=find_hfw_dist([male_corpus])\n",
    "some_words=display(hff_theft,words=function_words_theft,hfb=hfm_theft,leg=['Female speakers','Male speakers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are minimal differences between the distributions. 'of', 'to', 'him','he' and 'the' appear more often for male speakers.  Some punctuation and 'my' appears more often for female speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to find the characteristic words for a corpus (i.e., words which occur more often than one would expect by chance).  We can do this using PMI and/or likelihood lift ratio - but it is conventional in work on comparing corpora to use log-likelihood-ratio (LLR (Dunning, 1993)) as this is less biased towards low frequency events than PMI and doesn't require any parameters to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makedict(alist):\n",
    "    adict={}\n",
    "    for (key,value) in alist:\n",
    "        adict[key]=adict.get(key,0)+value\n",
    "    return adict\n",
    "        \n",
    "\n",
    "def pmi(wordfreq,refwordfreq,corpussize,refcorpussize):\n",
    "    if wordfreq*refcorpussize*refwordfreq*corpussize==0:\n",
    "        score=0\n",
    "#        print(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    else:\n",
    "        score=np.log((wordfreq*refcorpussize)/(refwordfreq*corpussize))\n",
    "    return score\n",
    "\n",
    "\n",
    "def rev_pmi(wordfreq,refwordfreq,corpussize,refcorpussize):\n",
    "    return pmi(refwordfreq-wordfreq, refwordfreq,refcorpussize-corpussize,refcorpussize)\n",
    "        \n",
    "def llr(wordfreq,refwordfreq,corpussize,refcorpussize):\n",
    "    #print(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    mypmi=pmi(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    myrevpmi=rev_pmi(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    #myrevpmi2=rev_pmi2(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    #print(mypmi,myrevpmi,myrevpmi2)\n",
    "    llr_score=2*(wordfreq*mypmi+(refwordfreq-wordfreq)*myrevpmi)\n",
    "    if pmi(wordfreq,refwordfreq,corpussize,refcorpussize)<0:\n",
    "        return -llr_score\n",
    "    else:\n",
    "        return llr_score\n",
    "    \n",
    "def likelihoodlift(wordfreq,refwordfreq,corpussize,refcorpussize,alpha):\n",
    "    beta=0\n",
    "    if alpha==1:\n",
    "        return math.log(wordfreq/corpussize)\n",
    "    elif alpha==0:\n",
    "        return pmi(wordfreq,refwordfreq,corpussize,refcorpussize)\n",
    "    else:\n",
    "        return(alpha*math.log(beta+(wordfreq/corpussize))+(1-alpha)*pmi(wordfreq,refwordfreq,corpussize,refcorpussize))\n",
    "\n",
    "def mysurprise(wf,rwf,cs,rcs,measure,params):\n",
    "    if measure=='pmi':\n",
    "        return pmi(wf,rwf,cs,rcs)\n",
    "    elif measure=='llr':\n",
    "        return llr(wf,rwf,cs,rcs)\n",
    "    elif measure =='likelihoodlift':\n",
    "        return likelihoodlift(wf,rwf,cs,rcs,params.get('alpha',0.5))\n",
    "    else:\n",
    "        print(\"Unknown measure of surprise\")\n",
    "\n",
    "def improved_compute_surprises(corpusA,corpusB,measure,params={}):\n",
    "    (corpusAsize,wordlistA)=corpusA\n",
    "    (corpusBsize,wordlistB)=corpusB\n",
    "    if 'threshold' in params.keys():\n",
    "        threshold=params['threshold']\n",
    "    else:\n",
    "        threshold=len(wordlistA)\n",
    "    #dictA=makedict(wordlistA)\n",
    "    dictB=makedict(wordlistB)\n",
    "    \n",
    "    scores=[]\n",
    "   # print(wordlistA[:threshold])\n",
    "    for(term,freq) in wordlistA[:threshold]:\n",
    "        scores.append((term,mysurprise(freq,dictB.get(term,freq+1),corpusAsize,corpusBsize,measure,params)))\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True)\n",
    "    k=20\n",
    "    print(\"Top {} terms are \".format(k))\n",
    "    print(sortedscores[:k])\n",
    "    rank=0\n",
    "    if measure==\"llr\":\n",
    "        for (term,score) in sortedscores:\n",
    "            if score>10.828:\n",
    "                rank+=1\n",
    "            else:\n",
    "                break\n",
    "        print(\"{} significantly characterising terms\".format(rank))\n",
    "    return(sortedscores[:rank])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the LLR to compute the most characteristic words for males and for females.  Any with a score over 10.828 are statistically significant at the 0.1% level.  We display the top 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malewords=improved_compute_surprises(hfm_theft,hfw_theft,'llr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "femalewords=improved_compute_surprises(hff_theft,hfw_theft,'llr',params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we can use the likelihood-lift ratio - however now we need to set alpha to decide the weight of likelihood and lift respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_compute_surprises(hfm_theft,hfw_theft,'likelihoodlift',params={'alpha':0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_compute_surprises(hff_theft,hfw_theft,'likelihoodlift',params={'alpha':0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Different Periods Corresponding to Different Scribes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have 3 subperiods within the 1800-1820 period which correspond to 3 different scribes / publishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_nonlegal=['corpus_theft_def_1800_1805','corpus_theft_wv_1800_1805']\n",
    "p2_nonlegal=['corpus_theft_def_1806_1815','corpus_theft_wv_1806_1815']\n",
    "p3_nonlegal=['corpus_theft_def_1816_1820','corpus_theft_wv_1816_1820']\n",
    "\n",
    "p1_corpus=nlp_tools.corpus(p1_nonlegal,nlp,prop=p,ner=False)\n",
    "p2_corpus=nlp_tools.corpus(p2_nonlegal,nlp,prop=p,ner=False)\n",
    "p3_corpus=nlp_tools.corpus(p3_nonlegal,nlp,prop=p,ner=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_corpus=nlp_tools.corpus(p1_nonlegal+p2_nonlegal+p3_nonlegal,nlp,prop=p,ner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autolabel(rects,ax):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    \n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height*1.1,\n",
    "                '%1.1f' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "def display_list(hfw_list,cutoff=10,words=[],leg=None,title=None):\n",
    "    width=0.7/len(hfw_list)\n",
    "    toplot=[]\n",
    "    for hfw in hfw_list:\n",
    "        corpussize=hfw[0]\n",
    "        if words==[]:\n",
    "            todisplay=hfw[1][:cutoff]\n",
    "        else:\n",
    "            todisplay=[(x,y) for (x,y) in hfw[1] if x in words]\n",
    "            cutoff=len(words)\n",
    "        barvalues=sorted(todisplay,key=operator.itemgetter(0),reverse=False)\n",
    "        #print(barvalues)\n",
    "        xs,ys=[*zip(*barvalues)]\n",
    "        ps=[y*100/corpussize for y in ys]\n",
    "    \n",
    "        toplot.append(ps)\n",
    "        \n",
    "    N=cutoff\n",
    "    ind=np.arange(N)\n",
    "    fig,ax=plt.subplots(figsize=(2*cutoff,cutoff/2))\n",
    "    rectset=[]\n",
    "    colors=['r','b','y','g']\n",
    "    for i,ps in enumerate(toplot):\n",
    "        rectset.append(ax.bar(ind+i*width,ps,width,color=colors[i]))\n",
    "    \n",
    "    if leg!=None:\n",
    "        ax.legend(rectset,leg)\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(xs)\n",
    "    ax.set_xlabel('High Frequency Words')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim(0,10)\n",
    "    for rects in rectset:\n",
    "        autolabel(rects,ax)\n",
    "    if title!=None:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    return xs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the high frequency words across the whole corpus (which approximates the male+female combination in the last section - this is slightly larger, due presumably to some utterances not being labelled as either female or male)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100000\n",
    "hfperiods_theft=find_hfw_dist([p1_corpus,p2_corpus,p3_corpus],k=k)\n",
    "function_words_theft=display_list([hfperiods_theft],cutoff=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hfp1_theft=find_hfw_dist([p1_corpus],k=k)\n",
    "hfp2_theft=find_hfw_dist([p2_corpus],k=k)\n",
    "hfp3_theft=find_hfw_dist([p3_corpus],k=k)\n",
    "some_words=display_list([hfp1_theft,hfp2_theft,hfp3_theft],words=function_words_theft,leg=['1800-1805','1806-1815','1816-1820'],title=\"High Frequency Word Distributions for Non_legal Speakers in Theft Cases Split by Transcription Period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the different corpora, we see that there is a much greater use of ; in the first period and correspondingly, a greater use of . in the later periods.  We also see an increasing use of 'the' throughout the 3 periods and a corresponding decreasing use of 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1words=improved_compute_surprises(hfp1_theft,hfperiods_theft,'llr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2words=improved_compute_surprises(hfp2_theft,hfperiods_theft,'llr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3words=improved_compute_surprises(hfp3_theft,hfperiods_theft,'llr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Characterising Terms\n",
    "\n",
    "Having derived a set of characterising terms for a subcorpus, we want to analyse them.  Basic analysis includes average frequency and POS tag distribution.  Further analysis includes clustering semantically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autolabel(rects,ax):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    \n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height*1.1,\n",
    "                '%1.1f' % height,\n",
    "                ha='center', va='bottom')\n",
    "        \n",
    "def improved_display_list(xvalues,yvalueslist,labels={}):\n",
    "    width=0.7/len(yvalueslist)      \n",
    "    N=len(xvalues)\n",
    "    ind=np.arange(N)\n",
    "    fig,ax=plt.subplots(figsize=(20,12))\n",
    "    rectset=[]\n",
    "    colors=['r','b','y','g']\n",
    "    for i,ps in enumerate(yvalueslist):\n",
    "        rectset.append(ax.bar(ind+i*width,ps,width,color=colors[i]))\n",
    "    \n",
    "    leg=labels.get('leg',None)\n",
    "    title=labels.get('title',None)\n",
    "    xlabel=labels.get('xlabel','Year')\n",
    "    ylabel=labels.get('ylabel','Probability')\n",
    "    ylim=labels.get('ylim',1)\n",
    "    if leg!=None:\n",
    "        ax.legend(rectset,leg)\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(xvalues)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim(0,ylim)\n",
    "    for rects in rectset:\n",
    "        autolabel(rects,ax)\n",
    "    if title!=None:\n",
    "        ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have a corpus e.g., male_corpus and a set of characterising terms for that corpus e.g., malewords\n",
    "def find_pos(term,corpus):\n",
    "    pospos=['NOUN','VERB','ADJ','ADV','PUNCT']\n",
    "    counts={}\n",
    "    for apos in pospos:\n",
    "        counts[apos]=corpus.wordposdict.get((term,apos),0)\n",
    "    \n",
    "    total=sum(counts.values())\n",
    "    \n",
    "    gt=corpus.allworddict.get(term,0)\n",
    "    counts['OTHER']=gt-total\n",
    "    #print(term,gt,counts)\n",
    "    if gt>0:\n",
    "        poses=[(tag,weight/gt) for (tag,weight) in counts.items()]\n",
    "    else:\n",
    "        poses=[]\n",
    "    #print(term,poses)\n",
    "    return poses\n",
    "    \n",
    "def analyse(termset,corpus):\n",
    "    freqs=[]\n",
    "    somefreqs=[]\n",
    "    posdict={}\n",
    "    someposdict={}\n",
    "    threshold=20\n",
    "    for i,(term,relevance) in enumerate(termset):\n",
    "        freq=corpus.allworddict[term]\n",
    "        freqs.append(freq)\n",
    "        if i<threshold:\n",
    "            somefreqs.append(freq)\n",
    "        poses=find_pos(term,corpus)\n",
    "        for mypos,weight in poses:\n",
    "            posdict[mypos]=posdict.get(mypos,0)+weight\n",
    "            if i<threshold:\n",
    "                someposdict[mypos]=someposdict.get(mypos,0)+weight\n",
    "            \n",
    "        \n",
    "    freqarray=np.array(freqs)    \n",
    "    meanfreq=np.mean(freqarray)\n",
    "    sdfreq=np.std(freqarray)\n",
    "    meanprob=meanfreq/corpus.wordtotal\n",
    "    sdprob=sdfreq/corpus.wordtotal\n",
    "    print(\"Mean frequency is {}, sd is {}\".format(meanfreq,sdfreq))\n",
    "    print(\"Mean probability is {}, sd is {}\".format(meanprob,sdprob))\n",
    "    somefreqarray=np.array(somefreqs)\n",
    "    meansomefreq=np.mean(somefreqarray)\n",
    "    sdsomefreq=np.std(somefreqarray)\n",
    "    meansomeprob=meansomefreq/corpus.wordtotal\n",
    "    sdsomeprob=sdsomefreq/corpus.wordtotal\n",
    "    print(\"For top {} words, mean freq is {}, sd is {}\".format(threshold,meansomefreq,sdsomefreq))\n",
    "    print(\"For top {} words, mean prob is {}, sd is {}\".format(threshold,meansomeprob,sdsomeprob))\n",
    "    #print(posdict)\n",
    "    xvalues=posdict.keys()\n",
    "    totaly=sum(posdict.values())\n",
    "    totalz=sum(someposdict.values())\n",
    "    allvalues=[]\n",
    "    somevalues=[]\n",
    "    for x in xvalues:\n",
    "        allvalues.append(posdict.get(x,0))\n",
    "        somevalues.append(someposdict.get(x,0))\n",
    "    yvalues=[[100*y/totaly for y in allvalues],[100*z/totalz for z in somevalues]]\n",
    "    labels={'title':'Distribution of POS in Characterising Terms','xlabel':'Part of Speech','ylabel':'Proportion','leg':['Whole Set',\"Top {}-restricted Set\".format(threshold)],'ylim':100}\n",
    "    improved_display_list(xvalues,yvalues,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(malewords,theft_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(femalewords,theft_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(p1words,theft_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(p2words,theft_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(p3words,theft_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a much higher percentage of the characteristic words in the period corpora are punctuation and function words.  In the male/female splits, a higher percentage of the characteristic words are content words.  Looking at the most characterising words, it seems that there may be more characteristic nouns for females and more characteristic verbs for males but this would need further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "\n",
    "#parameters\n",
    "size=300\n",
    "min_count=10\n",
    "window=1\n",
    "sg=1 #1=use skip-gram, otherwise=cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "theft_model=Word2Vec(theft_corpus.sentences,min_count=min_count,window=window,size=size,sg=sg,workers=4)\n",
    "vocab=list(theft_model.wv.vocab.keys())\n",
    "print(\"Word2Vec model built with example vocabulary:\")\n",
    "print(str(vocab[0:10]))\n",
    "end=time.time()\n",
    "print(\"Time taken: {}s\".format(str(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_model.wv.most_similar(['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_model.wv.most_similar(['prisoner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_model.wv.most_similar(['warehouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_model.wv.most_similar(['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbours(wordset,w2vmodel):\n",
    "    threshold=20\n",
    "    found=0\n",
    "    for i,(term,score) in enumerate(wordset):\n",
    "        try:\n",
    "            neighbours=w2vmodel.wv.most_similar([term])\n",
    "            found+=1\n",
    "            if i<threshold:\n",
    "                print(term,neighbours)\n",
    "        except:\n",
    "            print(\"{} not in vocab\".format(term))\n",
    "    \n",
    "    oov=100-(found*100/len(wordset))\n",
    "    print(\"Out of vocabulary: {}\".format(oov))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours(femalewords,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours(malewords,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours(p1words,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours(p2words,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours(p3words,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_matrix(wordset,model,threshold=0.5):\n",
    "    \n",
    "    matrix=[]\n",
    "    \n",
    "    for (termA,_score) in wordset:\n",
    "        row=[]\n",
    "        for(termB,_score) in wordset:\n",
    "            try:\n",
    "                sim=model.wv.similarity(termA,termB)\n",
    "                if sim<threshold:\n",
    "                    sim=0\n",
    "            except:\n",
    "                sim=0\n",
    "            row.append(sim)\n",
    "            \n",
    "        matrix.append(row)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_matrix=make_matrix(femalewords,theft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "punctdict={\"\\n\":\"_NEWLINE\",\";\":\"_SEMICOLON\",\":\":\"_COLON\",\"\\\"\":\"_QUOTE\",\"'s\":\"_GEN\",\"-\":\"_HYPHEN\",\"(\":\"_LEFTBRACKET\",\")\":\"_RIGHTBRACKET\",\",\":\"_COMMA\",\".\":\"_FULLSTOP\",\"..\":\"_DOTDOT\"}\n",
    "\n",
    "def clean(term):\n",
    "    \n",
    "    #remove punctuation which will confuse Gephi\n",
    "    cleanterm=punctdict.get(term,term)\n",
    "    return cleanterm\n",
    "\n",
    "def make_csv(wordset,model,filename):\n",
    "    matrix=make_matrix(wordset,model)\n",
    "    terms=[clean(term) for (term,score) in wordset]\n",
    "    \n",
    "    #with open(filename,'w') as csvfile:\n",
    "    #    csvwriter=csv.writer(csvfile,dialect='excel')\n",
    "    #    headings=['']+terms\n",
    "        #print(headings)\n",
    "    #    csvwriter.writerow(headings)\n",
    "    #    for term,row in zip(terms,matrix):\n",
    "    #        csvwriter.writerow([term]+row)\n",
    "     \n",
    "    with open(filename,'w') as csvfile:\n",
    "        line=\"\"\n",
    "        for term in terms:\n",
    "            line+=';'+term\n",
    "        line+='\\n'\n",
    "        \n",
    "        csvfile.write(line)\n",
    "        #print(line)\n",
    "        for term,row in zip(terms,matrix):\n",
    "            line=term\n",
    "            #print(row)\n",
    "            for item in row:\n",
    "                line+=';'+str(item)\n",
    "                \n",
    "            line+='\\n'\n",
    "            \n",
    "            csvfile.write(line)\n",
    "            #print(line) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_csv(femalewords,theft_model,'matrix_female.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_csv(malewords,theft_model,'matrix_male.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_csv(p1words,theft_model,'matrix_p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_csv(p2words,theft_model,'matrix_p2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_csv(p3words,theft_model,'matrix_p3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(malewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theft_lexicon=makedict(hfperiods_theft[1])\n",
    "p1theft_lexicon=makedict(hfp1_theft[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup(term,d1,d2):\n",
    "    lex1=makedict(d1[1])\n",
    "    lex2=makedict(d2[1])\n",
    "    print('{}:{},{}'.format(term,lex1[term],lex2[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup('fork',hfperiods_theft,hfp1_theft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup('drawer',hfw_theft,hff_theft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
